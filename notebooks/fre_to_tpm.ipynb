{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire Radiative Energy to Total Particulate Matter Workbook\n",
    "\n",
    "This workbook develops the workflow for generating estiamtes of total particulate matter (TPM) from observations of fire radiative energy (FRE).  The FRE observations are derived from time integrated observations of fire radiative power (FRP), which is a measure of the amount of energy emitted by a fire per unit time and has units $Wm^{-2}$.  In order integrate over time we need observations of the FRP through time, and therefore rely on observations of fires from geostrationary sensors, in particular: SEVIRI over Africa/Europe; GOES over North and South America; Himawari over Asia and Oceania.  Which provide observations at 15, 30 and 10 minute intervals respectively.  \n",
    "\n",
    "The TPM ($gm^{-2}$) observations are required to be instantaneous only, as such they can be derived from polar orbiting sensors.   The TPM estimates are derived from optical depth (OD, unitless) estimates taken from the smoke plumes associated with a fire as the ratio OD to the mass extinction coffecient ($m^{2}g^{-1}$).  As such for TPM we need two parameters.  OD is a commonly retrived parameters from remotely sensed optical imagery, for example MODIS has a number of products which come under the MOD04/MYD04 product classification.  The mass extinction coefficient is typically derived from in-situ observations. \n",
    "\n",
    "The basic premise is to develop FRE observations for a set of fires, specifcally selecting fires with clear and definable smoke plumes, calculating the TPM for these plumes and finally deriving a linear relation between the FRE and the TPM.  Once this relation is defined it is then possible to determine the TPM directly from observations of FRE.\n",
    "\n",
    "Here we explore the development of this relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in required packages\n",
    "import ast\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "from pyhdf.SD import SD, SDC\n",
    "from matplotlib.path import Path\n",
    "from scipy import stats\n",
    "from scipy import integrate\n",
    "from shapely.geometry import Polygon, Point\n",
    "from shapely.ops import transform\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import pyresample as pr\n",
    "\n",
    "import src.config.filepaths as filepaths\n",
    "import src.config.sensor as sensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_path = '/Users/dnf/Projects/kcl-fire-aot/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the FRP for the himawair sensor (as currently working on SE Asia).  Keep only the columns we need, convert lat lon columns to a shapely Point class, and set the index to datetime.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_frp():\n",
    "    if sensor.sensor == 'himawari':\n",
    "        try:\n",
    "            frp_df = pd.read_pickle(filepaths.path_to_himawari_frp + 'himawari_df.p')\n",
    "        except Exception, e:\n",
    "            print('could not load frp dataframe, failed with error ' + str(e) + ' building anew')\n",
    "            frp_files = os.listdir(filepaths.path_to_himawari_frp)\n",
    "            df_from_each_file = (pd.read_csv(os.path.join(filepaths.path_to_himawari_frp, f)) for f in frp_files)\n",
    "            frp_df = pd.concat(df_from_each_file, ignore_index=True)\n",
    "\n",
    "            # lets dump the columns we don't want\n",
    "            frp_df = frp_df[['FIRE_CONFIDENCE', 'FRP_0', 'LATITUDE', 'LONGITUDE', 'year','month','day','time']]\n",
    "\n",
    "            # make geocoords into shapely points\n",
    "            points = [Point(p[0], p[1]) for p in zip(frp_df['LONGITUDE'].values, frp_df['LATITUDE'].values)]\n",
    "            frp_df['point'] = points\n",
    "            frp_df.drop(['LONGITUDE', 'LATITUDE'], axis=1, inplace=True)\n",
    "\n",
    "            # reindex onto date\n",
    "            for k in ['year', 'month', 'day', 'time']:\n",
    "                frp_df[k] = frp_df[k].astype(int).astype(str)\n",
    "                if k == 'time':\n",
    "                    frp_df[k] = frp_df[k].str.zfill(4)\n",
    "                if k in ['month', 'day']:\n",
    "                    frp_df[k] = frp_df[k].str.zfill(2)\n",
    "\n",
    "            format = '%Y%m%d%H%M'\n",
    "            frp_df['obs_time'] = pd.to_datetime(frp_df['year'] +\n",
    "                                             frp_df['month'] +\n",
    "                                             frp_df['day'] + \n",
    "                                             frp_df['time'], format=format)\n",
    "            frp_df['obs_date'] = frp_df['obs_time'].dt.date\n",
    "            frp_df.drop(['year','month','day', 'time'], axis=1, inplace=True)\n",
    "            \n",
    "            frp_df.to_pickle(filepaths.path_to_himawari_frp + 'himawari_df.p')\n",
    "\n",
    "    elif sensor.sensor == 'goes':\n",
    "        pass\n",
    "\n",
    "    return frp_df\n",
    "frp_data = load_frp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_polygon(plume, orac_data):\n",
    "\n",
    "    # TODO replace this with ORAC data, not using L1B data\n",
    "    # get geographic coordinates of plume bounds (first test with l2 data)\n",
    "    myd_data = SD(os.path.join(filepaths.path_to_modis_l1b, plume.filename), SDC.READ)\n",
    "    lats = ndimage.zoom(myd_data.select('Latitude').get(), 5)\n",
    "    lons = ndimage.zoom(myd_data.select('Longitude').get(), 5)\n",
    "    \n",
    "    # when digitising points are appended (x,y).  However, arrays are accessed\n",
    "    # in numpy as row, col which is y, x.  So we need to switch\n",
    "    bounding_lats = [lats[point[1], point[0]] for point in plume.plume_extent]\n",
    "    bounding_lons = [lons[point[1], point[0]] for point in plume.plume_extent]\n",
    "\n",
    "    return Polygon(zip(bounding_lons, bounding_lats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the smoke plume mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask_path = root_path + 'Asia/processed/plume_masks/myd021km_plumes_df.pickle'\n",
    "try:\n",
    "    mask_df = pd.read_pickle(mask_path)\n",
    "except:\n",
    "    mask_df = pd.read_csv(mask_path, quotechar='\"', sep=',', converters={'plume_extent': ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_fre(plume, frp_data):\n",
    "        \n",
    "    # subset temporally, getting day of overpass and also day before\n",
    "    plume_date_stop = datetime.strptime(plume.filename[10:17], '%Y%j').date()\n",
    "    plume_date_start = plume_date_stop - timedelta(days=1)\n",
    "    try:\n",
    "        frp_subset = frp_data.loc[(frp_data['obs_date'] == plume_date_stop) | \n",
    "                                   (frp_data['obs_date'] == plume_date_start)]\n",
    "    except Exception, e:\n",
    "        print 'Could not extract time subset, failed with error:', str(e)\n",
    "        return None\n",
    "        \n",
    "    # subset spatially finding only those fires within the bounds of the plume\n",
    "    # note Matplotlib path might be a better option to check with bounds\n",
    "    # see here: https://goo.gl/Cevi1u\n",
    "    inbounds = []\n",
    "    plume_polygon = build_polygon(plume, [])\n",
    "    try:\n",
    "        for i, (index, frp_pixel) in enumerate(frp_subset.iterrows()):\n",
    "            if frp_pixel['point'].within(plume_polygon):\n",
    "                inbounds.append(i)\n",
    "        if inbounds:\n",
    "            frp_subset = frp_subset.iloc[inbounds]\n",
    "    except Exception, e:\n",
    "        print 'Could not extract spatial subset, failed with error:', str(e)\n",
    "        return None\n",
    "           \n",
    "    # group by time and then aggregate the FRP variables appropriately\n",
    "    frp_subset['FIRE_CONFIDENCE_mean'] = frp_subset['FIRE_CONFIDENCE']\n",
    "    frp_subset['FIRE_CONFIDENCE_std'] = frp_subset['FIRE_CONFIDENCE']\n",
    "    frp_subset = frp_subset.groupby('obs_time').agg({'FRP_0':np.sum, \n",
    "                                                     'FIRE_CONFIDENCE_mean':np.mean,\n",
    "                                                    'FIRE_CONFIDENCE_std':np.std})[['FRP_0', \n",
    "                                                                                    'FIRE_CONFIDENCE_mean',\n",
    "                                                                                    'FIRE_CONFIDENCE_std']]\n",
    "    \n",
    "    # get the plume observation overpass UTC time\n",
    "    plume_obs_time = datetime.strptime(re.search('[0-9]{7,7}\\.[0-9]{4,4}', plume.filename).group(0), '%Y%j.%H%M')\n",
    "    plume_obs_time_less_12_hours = plume_obs_time - timedelta(hours=12)\n",
    "    \n",
    "    # we can then get all FRPs in the 12 hours before the overpass\n",
    "    frp_subset = frp_subset[(frp_subset.index <= plume_obs_time) & \n",
    "                            (frp_subset.index >= plume_obs_time_less_12_hours)]\n",
    "    \n",
    "    # set up the times for integration\n",
    "    try:\n",
    "        t0 = frp_subset.index[0]\n",
    "        sample_times = (frp_subset.index - t0).total_seconds()\n",
    "    except Exception, e:\n",
    "        print 'Could not extract spatial subset, failed with error:', str(e)\n",
    "        return None\n",
    " \n",
    "    # now integrate \n",
    "    fre = integrate.trapz(frp_subset['FRP_0'], sample_times)\n",
    "    print 'fre:', fre\n",
    "    return fre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_plume_area(plume):\n",
    "    \n",
    "    # first build shapely polygon\n",
    "    plume_polygon = build_polygon(plume, [])\n",
    "    \n",
    "    # TODO is sinusoidal proj good enough?  Need to switch to UTM\n",
    "    # see comments in here: https://goo.gl/SD8DMQ\n",
    "    m = Basemap(projection='sinu',lon_0=0,resolution='c')\n",
    "    \n",
    "    # apply to shapely polygon\n",
    "    projected_plume_polygon = transform(m, plume_polygon)\n",
    "    \n",
    "    # compute projected polygon area in m2\n",
    "    return projected_plume_polygon.area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_plume_mask(plume_extent):\n",
    "    \n",
    "    x, y = zip(*plume_extent)    \n",
    "    min_x, max_x = min(x), max(x)\n",
    "    min_y, max_y = min(y), max(y)\n",
    "\n",
    "    zeroed_x = np.array(x) - min_x\n",
    "    zeroed_y = np.array(y) - min_y\n",
    "    extent = zip(zeroed_x, zeroed_y)\n",
    "    \n",
    "    nx = max_x - min_x\n",
    "    ny = max_y - min_y\n",
    "\n",
    "    x, y = np.meshgrid(np.arange(nx), np.arange(ny))\n",
    "    x, y = x.flatten(), y.flatten()\n",
    "    points = np.vstack((x, y)).T\n",
    "\n",
    "    poly_verts = extent\n",
    "\n",
    "    # apply mask\n",
    "    path = Path(poly_verts)\n",
    "    mask = path.contains_points(points)\n",
    "    mask = mask.reshape((ny, nx))\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_aod():\n",
    "    \n",
    "    '''\n",
    "    Resampling of the ORAC AOD data is required to remove the bowtie effect from the data.  We can then\n",
    "    sum over the AODs contained with the plume.\n",
    "    \n",
    "    Resampling of the MODIS AOD data is required so that it is in the same projection as the ORAC AOD.  \n",
    "    With it being in the same projection we can replace low quality ORAC AODs with those from MXD04 products.  \n",
    "    We also need to get the background AOD data from MXD04 products as ORAC does not do optically thin\n",
    "    retrievals well.  \n",
    "    \n",
    "    Also, we need to check that the pixel area estimates being computed by compute_plume_area are reasonable.\n",
    "    That can be done in this function, we just produce another area estimate from the resampled mask by getting \n",
    "    vertices, getting the lat lons of the vertices, creating a shapely polygon from them and then computing the area.\n",
    "    '''\n",
    "    \n",
    "    # get plume image extent\n",
    "    x, y = zip(*plume.plume_extent)\n",
    "    min_x, max_x = min(x), max(x)\n",
    "    min_y, max_y = min(y), max(y)\n",
    "    \n",
    "    # build polygon mask\n",
    "    plume_mask = get_plume_mask(plume.plume_extent)\n",
    "    \n",
    "    # load MYD04 data \n",
    "    \n",
    "    # build polygon mask\n",
    "    \n",
    "    # subset out ORAC AOD, and geo grids\n",
    "    myd_data = SD(os.path.join(filepaths.path_to_modis_l1b, plume.filename), SDC.READ)\n",
    "    lats = ndimage.zoom(myd_data.select('Latitude').get(), 5)[min_y:max_y, min_x:max_x]\n",
    "    lons = ndimage.zoom(myd_data.select('Longitude').get(), 5)[min_y:max_y, min_x:max_x]\n",
    "        \n",
    "    # create map definitions from geo grids (do UTM at some point)\n",
    "    lat_r = np.arange(np.min(lats), np.max(lats), 0.01)\n",
    "    lon_r = np.arange(np.min(lons), np.max(lons), 0.01)\n",
    "    lon_r, lat_r = np.meshgrid(lon_r, lat_r)\n",
    "    \n",
    "    def_a = pr.geometry.SwathDefinition(lons=lons, lats=lats)\n",
    "    def_b = pr.geometry.SwathDefinition(lons=lon_r, lats=lat_r)\n",
    "    \n",
    "    # resample mask, ORAC AOD, ORAC costs, MYD04 AOD, MYD04 QA\n",
    "    resampled_mask = pr.kd_tree.resample_nearest(def_a,\n",
    "                                                 plume_mask,\n",
    "                                                 def_b,\n",
    "                                                 radius_of_influence=9000,\n",
    "                                                 fill_value=-999)\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    # create best AOD map from ORAC and MYD04 AOD\n",
    "    \n",
    "    # extract best AOD using plume mask\n",
    "    \n",
    "    # split into background and plume AODs\n",
    "    \n",
    "    # subtract background from plume\n",
    "    \n",
    "    # return plume AOD\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b90a9a717144>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# get plume AOD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtotal_aod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_aod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-e524e1f198af>\u001b[0m in \u001b[0;36mcompute_aod\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mmyd_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_to_modis_l1b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplume\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSDC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mlats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzoom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyd_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Latitude'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mlons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzoom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyd_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Longitude'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# create map definitions from geo grids (do UTM at some point)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dnf/anaconda2/envs/kcl-fire-aot/lib/python2.7/site-packages/scipy/ndimage/interpolation.pyc\u001b[0m in \u001b[0;36mzoom\u001b[0;34m(input, zoom, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    610\u001b[0m                                                    shape=output_shape)\n\u001b[1;32m    611\u001b[0m     \u001b[0mzoom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzoom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m     \u001b[0m_nd_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzoom_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzoom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, plume in mask_df.iterrows():\n",
    "    \n",
    "    # get collocated FRP data\n",
    "    #fre = compute_fre(plume, frp_data)\n",
    "    \n",
    "    # get area of the plume\n",
    "    #plume_area = compute_plume_area(plume)\n",
    "    \n",
    "    # get plume AOD\n",
    "    total_aod = compute_aod()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kcl-fire-aot",
   "language": "python",
   "name": "kcl-fire-aot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
